#!/usr/bin/env python3
"""
visualize_training.py - Streaming-basierte Visualisierung f√ºr massive Datasets
Implementiert Best Practices: Streaming, inkrementelle Aggregation, intelligentes Sampling

Basiert auf visualization_guidelines.md:
- Verarbeitet ALLE JSON-Dateien eines Training-Runs
- Verwendet Reservoir Sampling f√ºr gleichm√§√üige Datenverteilung  
- Memory-bounded Processing (konstant 10k Samples im RAM)
- Inkrementelle Aggregation f√ºr Position-Heatmaps
"""

import argparse
import sys
import gc
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from typing import Optional, List, Dict, Tuple
import json
from collections import defaultdict
import random

# Import utility modules
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from utils.data_loader import (
    find_latest_experiment_dir, 
    find_json_log_files, 
    flatten_dict,
    validate_dataframe
)
from utils.plot_helpers import (
    setup_plot_style,
    smooth_series,
    get_reward_columns,
    get_position_columns,
    create_plot_title,
    save_plot,
    apply_plot_styling,
    REWARD_COLORS,
    PLOT_CONFIG
)


class StreamingProcessor:
    """Streaming processor f√ºr massive JSON datasets mit intelligenter Aggregation"""
    
    def __init__(self, target_sample_size: int = 10000):
        self.target_sample_size = target_sample_size
        self.reservoir_samples = []  # Reservoir sampling buffer
        self.total_entries_seen = 0
        
        # Rolling statistics f√ºr inkrementelle Aggregation
        self.reward_stats = defaultdict(list)
        self.step_range = [float('inf'), float('-inf')]
        
        # Heatmap aggregation
        self.position_counts = defaultdict(int)
        
    def process_file_streaming(self, json_file: Path, debug_first_entry: bool = False) -> Tuple[int, Dict]:
        """
        Verarbeitet eine JSON-Datei mit Streaming und Reservoir Sampling
        
        Returns:
            (entries_count, file_stats)
        """
        entries_count = 0
        file_stats = {'rewards': {}, 'steps': [], 'positions': []}
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                return 0, file_stats
                
            entries_count = len(data)
            
            # Extrahiere globalen Timestamp aus Dateiname (z.B. stats_1753554387.json -> 1753554387)
            # NICHT MEHR VERWENDET - verwende direkt die Steps aus JSON
            import re
            timestamp_match = re.search(r'stats_(\d+)\.json', json_file.name)
            global_step = int(timestamp_match.group(1)) if timestamp_match else 0
            
            # Intelligentes Sampling: Zeitlich gleichverteilt
            # Nimm jeden N-ten Eintrag f√ºr gleichm√§√üige zeitliche Verteilung
            if entries_count > 1000:
                sample_step = max(1, entries_count // 100)  # Max 100 samples per file
                sampled_indices = list(range(0, entries_count, sample_step))
            else:
                sampled_indices = list(range(entries_count))
            
            for idx in sampled_indices:
                entry = data[idx]
                if isinstance(entry, dict):
                    flat_entry = flatten_dict(entry)
                    
                    # L√ñSUNG: Verwende Dateiname-Timestamp als echten globalen Step
                    flat_entry['global_step'] = global_step
                    
                    # Debug: Zeige ersten Eintrag
                    if debug_first_entry and idx == sampled_indices[0]:
                        print(f"üîç Debug - Erste 10 Keys des geflatteten Eintrags:")
                        for i, (key, value) in enumerate(list(flat_entry.items())[:10]):
                            print(f"   '{key}': {value}")
                        print(f"üîç Debug - Lokaler Step aus JSON: {flat_entry.get('step', 'N/A')}")
                        print(f"üîç Debug - Globaler Step aus Dateiname: {global_step:,}")
                        print(f"üîç Debug - Verwendet f√ºr X-Achse: {flat_entry.get('global_step', 'N/A'):,}")
                        print(f"üîç Debug - Alle Keys die 'step' enthalten:")
                        step_keys = [k for k in flat_entry.keys() if 'step' in k.lower()]
                        for key in step_keys:
                            print(f"   '{key}': {flat_entry[key]}")
                        print(f"üîç Debug - Alle Keys die 'position' oder 'x'/'y' enthalten:")
                        pos_keys = [k for k in flat_entry.keys() if any(pos in k.lower() for pos in ['position', 'x', 'y'])]
                        for key in pos_keys:
                            print(f"   '{key}': {flat_entry[key]}")
                    
                    # Reservoir Sampling f√ºr finale Visualisierung
                    self._reservoir_sample(flat_entry)
                    
                    # Inkrementelle Statistiken
                    self._update_rolling_stats(flat_entry)
                    
        except Exception as e:
            print(f"Fehler beim Streaming von {json_file.name}: {e}")
            return 0, file_stats
        
        # Garbage Collection nach jeder Datei
        gc.collect()
        return entries_count, file_stats
    
    def _reservoir_sample(self, entry: Dict):
        """Implementiert Reservoir Sampling f√ºr gleichm√§√üige Verteilung"""
        self.total_entries_seen += 1
        
        if len(self.reservoir_samples) < self.target_sample_size:
            # F√ºlle Reservoir auf
            self.reservoir_samples.append(entry)
        else:
            # Reservoir ist voll: Ersetze zuf√§llig
            j = random.randint(0, self.total_entries_seen - 1)
            if j < self.target_sample_size:
                self.reservoir_samples[j] = entry
    
    def _update_rolling_stats(self, entry: Dict):
        """Aktualisiert Rolling Statistics inkrementell"""
        # Step-Bereich tracking - verwende globale Steps aus Dateiname
        step_val = None
        
        # Priorisiere global_step (aus Dateiname) f√ºr echte Training-Steps
        if 'global_step' in entry:
            step_val = entry['global_step']
        elif 'step' in entry:
            step_val = entry['step']
        else:
            # Fallback: Suche nach step-enthaltenden Keys
            step_cols = [col for col in entry.keys() if 'step' in col.lower()]
            if step_cols:
                step_val = entry.get(step_cols[0])
        
        if step_val is not None and isinstance(step_val, (int, float)):
            self.step_range[0] = min(self.step_range[0], step_val)
            self.step_range[1] = max(self.step_range[1], step_val)
        
        # Position-Heatmap aggregation - nach flattening sollten Keys wie 'position_x', 'position_y' existieren
        x_val, y_val = None, None
        
        # Variante 1: Geflattete Position-Keys
        if 'position_x' in entry and 'position_y' in entry:
            x_val = entry['position_x']
            y_val = entry['position_y']
        
        # Variante 2: Andere m√∂gliche Formate nach flattening
        if x_val is None or y_val is None:
            for key in entry.keys():
                key_lower = key.lower()
                if ('position' in key_lower and 'x' in key_lower) and x_val is None:
                    x_val = entry.get(key)
                elif ('position' in key_lower and 'y' in key_lower) and y_val is None:
                    y_val = entry.get(key)
                
        # Variante 3: Direkte x/y Suche
        if x_val is None or y_val is None:
            if 'x' in entry and x_val is None:
                x_val = entry['x']
            if 'y' in entry and y_val is None:
                y_val = entry['y']
        
        # Speichere Position wenn beide Koordinaten verf√ºgbar
        if x_val is not None and y_val is not None:
            try:
                x_int = int(x_val)
                y_int = int(y_val)
                self.position_counts[(x_int, y_int)] += 1
            except (ValueError, TypeError):
                pass  # Ignoriere ung√ºltige Koordinaten
    
    def get_final_dataframe(self) -> pd.DataFrame:
        """Konvertiert Reservoir Samples zu DataFrame"""
        if not self.reservoir_samples:
            raise ValueError("Keine Samples gesammelt")
        
        df = pd.DataFrame(self.reservoir_samples)
        
        # Sortiere nach Steps - verwende globale Steps aus Dateiname
        step_cols = [col for col in df.columns if 'global_step' in col.lower()]
        if not step_cols:
            # Fallback auf normale Steps
            step_cols = [col for col in df.columns if 'step' in col.lower()]
        
        if step_cols:
            step_col = step_cols[0]
            df = df.sort_values(step_col)
        else:
            raise ValueError("Keine Step-Spalte gefunden")
        
        return df
    
    def get_statistics(self) -> Dict:
        """Gibt gesammelte Statistiken zur√ºck"""
        return {
            'total_entries_processed': self.total_entries_seen,
            'sample_size': len(self.reservoir_samples),
            'step_range': self.step_range if self.step_range[0] != float('inf') else [0, 0],
            'position_count': len(self.position_counts),
            'position_data': dict(self.position_counts),
            'sampling_ratio': len(self.reservoir_samples) / max(1, self.total_entries_seen) * 100
        }


def process_dataset_streaming(variant: str, max_files: Optional[int], verbose: bool) -> Tuple[pd.DataFrame, Path, str, Dict]:
    """
    Streaming-basierte Verarbeitung des gesamten Datasets
    
    Returns:
        (dataframe, experiment_dir, timestamp, statistics)
    """
    if verbose:
        print(f"üìÅ Suche neuestes Experiment f√ºr {variant}...")
    
    # Finde neuestes Experiment
    experiment_dir = find_latest_experiment_dir(variant)
    timestamp = experiment_dir.name
    
    if verbose:
        print(f"   Gefunden: {experiment_dir}")
    
    # Finde JSON-Dateien
    json_files = find_json_log_files(experiment_dir)
    
    if max_files:
        json_files = json_files[:max_files]
        print(f"‚ö†Ô∏è  Verwende nur {max_files} von {len(json_files)} Dateien")
    
    print(f"üåä Streaming-Verarbeitung von {len(json_files)} JSON-Dateien")
    
    # Initialisiere Streaming Processor
    processor = StreamingProcessor(target_sample_size=10000)
    
    total_entries = 0
    processed_files = 0
    
    # Verarbeite Dateien sequenziell mit Streaming
    for i, json_file in enumerate(json_files):
        if verbose or (i + 1) % 50 == 0:
            print(f"üìÑ Streaming Datei {i+1}/{len(json_files)}: {json_file.name}")
        
        # Debug nur f√ºr die erste Datei
        entries_count, _ = processor.process_file_streaming(json_file, debug_first_entry=(i == 0 and verbose))
        total_entries += entries_count
        processed_files += 1
        
        # Fortschritts-Update
        if (i + 1) % 100 == 0:
            stats = processor.get_statistics()
            print(f"   Fortschritt: {processed_files}/{len(json_files)} Dateien, "
                  f"{stats['total_entries_processed']:,} Eintr√§ge verarbeitet")
            print(f"   Aktueller Step-Bereich: {stats['step_range'][0]:,} bis {stats['step_range'][1]:,}")
            print(f"   Position-Daten bisher: {stats['position_count']:,} eindeutige Positionen")
    
    # Finale Statistiken
    stats = processor.get_statistics()
    stats['total_files'] = len(json_files)
    stats['total_entries'] = total_entries
    
    print(f"‚úÖ Streaming abgeschlossen:")
    print(f"   üìä Dateien verarbeitet: {processed_files:,}")
    print(f"   üìà Gesamte Eintr√§ge: {total_entries:,}")
    print(f"   üéØ Reservoir Samples: {stats['sample_size']:,}")
    print(f"   üìâ Sampling Ratio: {stats['sampling_ratio']:.2f}%")
    print(f"   üïê Step-Bereich: {stats['step_range'][0]:,} bis {stats['step_range'][1]:,}")
    
    # Erstelle finalen DataFrame
    df = processor.get_final_dataframe()
    
    # Validiere Daten
    is_valid, missing = validate_dataframe(df)
    if not is_valid:
        print(f"‚ö†Ô∏è  Datenvalidierung: Fehlend: {missing}")
    
    return df, experiment_dir, timestamp, stats


def plot_rewards_streaming(df: pd.DataFrame, variant: str, timestamp: str, 
                          output_dir: Path, smooth_window: int, output_format: str, 
                          stats: Dict, verbose: bool):
    """Streaming-optimierte Reward-Visualisierung"""
    
    if verbose:
        print("üìà Erstelle Streaming-optimierten Reward-Plot...")
    
    setup_plot_style()
    reward_mapping = get_reward_columns(df)
    
    if not reward_mapping:
        raise ValueError("Keine Reward-Spalten gefunden")
    
    # Verwende originale Steps aus JSON (echte Training-Steps)
    step_cols = [col for col in df.columns if 'step' in col.lower()]
    if not step_cols:
        raise ValueError("Keine Step-Spalte gefunden")
    step_col = step_cols[0]
    
    # Erstelle Plot
    fig, ax = plt.subplots(figsize=PLOT_CONFIG['figsize'])
    
    # Sortiere nach Steps
    df_plot = df.sort_values(step_col)
    
    # Debug: √úberpr√ºfe Step-Bereich im DataFrame
    if verbose:
        print(f"üîç Debug DataFrame Step-Bereich:")
        print(f"   Min Step im DataFrame: {df_plot[step_col].min():,}")
        print(f"   Max Step im DataFrame: {df_plot[step_col].max():,}")
        print(f"   Anzahl Datenpunkte: {len(df_plot):,}")
        print(f"   Step-Spalte: '{step_col}'")
    
    # Plotte jede Reward-Komponente
    for reward_type, col_name in reward_mapping.items():
        if col_name in df_plot.columns:
            # Adaptive Gl√§ttung basierend auf Datengr√∂√üe
            adaptive_window = min(smooth_window, len(df_plot) // 20)
            smoothed_values = smooth_series(df_plot[col_name], adaptive_window)
            
            # Bestimme Farbe und Stil
            color = REWARD_COLORS.get(reward_type, '#888888')
            linestyle = '-' if reward_type == 'total' else '--'
            linewidth = 2.0 if reward_type == 'total' else 1.0
            alpha = 1.0 if reward_type == 'total' else 0.7
            
            ax.plot(
                df_plot[step_col], 
                smoothed_values, 
                label=f'{reward_type.title()} Reward',
                color=color,
                linestyle=linestyle,
                linewidth=linewidth,
                alpha=alpha
            )
    
    # Title mit korrekten Statistiken
    title = create_plot_title(variant, timestamp, 
                            f"Rewards (Reservoir Sample: {stats['sample_size']:,}/{stats['total_entries']:,})")
    apply_plot_styling(ax, title, "Training Steps", "Reward")
    
    # Explizite X-Achsen-Skalierung basierend auf tats√§chlichen Daten
    actual_min = df_plot[step_col].min()
    actual_max = df_plot[step_col].max()
    ax.set_xlim(actual_min, actual_max)
    
    # Formatiere X-Achsen-Ticks f√ºr gro√üe Zahlen
    if actual_max > 1000000000:
        # F√ºr Milliarden: zeige in B Format
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e9:.1f}B'))
    elif actual_max > 1000000:
        # F√ºr Millionen: zeige in M Format
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
    elif actual_max > 1000:
        # F√ºr Tausende: zeige in K Format
        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e3:.0f}K'))
    
    if verbose:
        print(f"   X-Achse gesetzt: {actual_min:,} bis {actual_max:,}")
    
    # Speichere Plot
    filename = f"{variant}_{timestamp}_rewards_streaming"
    save_plot(fig, output_dir, filename, [output_format])
    
    plt.close(fig)
    
    if verbose:
        print(f"   Streaming Reward-Plot erstellt: {filename}.{output_format}")


def plot_heatmap_streaming(stats: Dict, variant: str, timestamp: str, 
                          output_dir: Path, output_format: str, verbose: bool):
    """Streaming-optimierte Position-Heatmap aus aggregierten Daten"""
    
    if verbose:
        print("üó∫Ô∏è Erstelle Streaming-optimierte Position-Heatmap...")
    
    position_data = stats.get('position_data', {})
    
    if not position_data:
        print("‚ö†Ô∏è  Keine Position-Daten f√ºr Heatmap verf√ºgbar")
        return
    
    setup_plot_style()
    
    # Konvertiere Position-Daten zu Listen
    positions = list(position_data.keys())
    counts = list(position_data.values())
    
    x_coords = [pos[0] for pos in positions]
    y_coords = [pos[1] for pos in positions]
    
    # Erstelle Plot
    fig, ax = plt.subplots(figsize=PLOT_CONFIG['figsize'])
    
    # Logarithmische Farbskalierung f√ºr bessere Sichtbarkeit
    counts_log = np.log1p(counts)  # log1p(x) = log(1+x) f√ºr bessere Darstellung bei 0
    
    # Scatter plot mit Gr√∂√üe proportional zu H√§ufigkeit
    scatter = ax.scatter(
        x_coords, y_coords, 
        c=counts_log, 
        s=np.clip(counts_log * 20, 10, 200),  # Gr√∂√üe zwischen 10 und 200
        cmap='plasma',
        alpha=0.7,
        edgecolors='white',
        linewidth=0.5
    )
    
    # Colorbar
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Visit Frequency (log scale)', rotation=270, labelpad=20)
    
    # Title mit Statistiken
    total_visits = sum(counts)
    unique_positions = len(positions)
    title = create_plot_title(variant, timestamp, 
                            f"Position Heatmap ({unique_positions:,} unique positions, {total_visits:,} total visits)")
    
    apply_plot_styling(ax, title, "X Position", "Y Position")
    
    # Grid f√ºr bessere Orientierung
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')
    
    # Speichere Plot
    filename = f"{variant}_{timestamp}_heatmap_streaming"
    save_plot(fig, output_dir, filename, [output_format])
    
    plt.close(fig)
    
    if verbose:
        print(f"   Streaming Heatmap erstellt: {filename}.{output_format}")
        print(f"   Eindeutige Positionen: {unique_positions:,}")
        print(f"   Gesamtbesuche: {total_visits:,}")


def main():
    """
    Hauptfunktion f√ºr Streaming-basierte Verarbeitung
    
    Erstellt beide erforderlichen Visualisierungen:
    - Reward-Plot mit Reservoir-Sampling
    - Position-Heatmap mit inkrementeller Aggregation
    """
    parser = argparse.ArgumentParser(
        description="Streaming-basierte Visualisierung massiver Agent-Trainingsdaten (alle JSON-Dateien)"
    )
    parser.add_argument("--variant", choices=["v1", "v2", "v3", "v4"], required=True)
    parser.add_argument("--output-dir", type=str, default=None)
    parser.add_argument("--format", choices=["png", "pdf", "svg"], default="png")
    parser.add_argument("--smooth-window", type=int, default=100)
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--max-files", type=int, default=None)
    
    args = parser.parse_args()
    
    try:
        # Streaming-basierte Datenverarbeitung
        df, experiment_dir, timestamp, stats = process_dataset_streaming(
            args.variant, args.max_files, args.verbose
        )
        
        # Ausgabe-Verzeichnis
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir = experiment_dir / "plots"
        
        # Erstelle Plots
        plot_rewards_streaming(
            df, args.variant, timestamp, output_dir, 
            args.smooth_window, args.format, stats, args.verbose
        )
        
        plot_heatmap_streaming(
            stats, args.variant, timestamp, output_dir, 
            args.format, args.verbose
        )
        
        print(f"‚úÖ Streaming-Visualisierung abgeschlossen!")
        print(f"üìä Verarbeitet: {stats['total_entries']:,} Eintr√§ge aus {args.variant}")
        print(f"üéØ Vollst√§ndiger Step-Bereich: {stats['step_range'][0]:,} bis {stats['step_range'][1]:,}")
        print(f"üó∫Ô∏è Position-Daten: {stats['position_count']:,} eindeutige Positionen")
        print(f"üìÅ Plots gespeichert in: {output_dir}")
        
    except Exception as e:
        print(f"‚ùå Fehler: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
